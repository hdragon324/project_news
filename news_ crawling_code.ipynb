{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json  \n",
    "\n",
    "# URL 불러오기\n",
    "\n",
    "cursor = 0\n",
    "result = []\n",
    "for i in range(5):\n",
    "    BASE_URL = f'https://news.naver.com/section/template/SECTION_ARTICLE_LIST?sid=100&sid2=&cluid=&date=&next={cursor}'\n",
    "    response = requests.get(BASE_URL)\n",
    "    i = response.json()\n",
    "    text = i['renderedComponent']['SECTION_ARTICLE_LIST']\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    news_link = soup.select('.sa_text a[data-rank]')\n",
    "    cursor = soup.select_one('div._PERSIST_META')['data-cursor']\n",
    "    for link in news_link:\n",
    "        result.append(link.attrs['href'])   # URL들은 result에 리스트형태로 입력\n",
    "    \n",
    "# 제목과 본문 출력\n",
    "articles = []\n",
    "for j in result:\n",
    "    response = requests.get(j)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.select_one('h2.media_end_head_headline').text\n",
    "    content = soup.select_one('article#dic_area').text.strip().replace(\"\\n\",'')\n",
    "    article = {\n",
    "            \"제목\": title,\n",
    "            \"내용\": content\n",
    "        }\n",
    "    articles.append(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사가 'news_articles.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json  \n",
    "\n",
    "# URL 불러오기\n",
    "cursor = 0\n",
    "result = []\n",
    "for i in range(5):\n",
    "    BASE_URL = f'https://news.naver.com/section/template/SECTION_ARTICLE_LIST?sid=100&sid2=&cluid=&date=&next={cursor}'\n",
    "    response = requests.get(BASE_URL)\n",
    "    \n",
    "    # JSON 응답 처리\n",
    "    i = response.json()\n",
    "    text = i['renderedComponent']['SECTION_ARTICLE_LIST']\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "        \n",
    "    # 뉴스 링크 선택\n",
    "    news_link = soup.select('.sa_text a[data-rank]')\n",
    "        \n",
    "    # 커서 업데이트\n",
    "    cursor = soup.select_one('div._PERSIST_META')['data-cursor']\n",
    "        \n",
    "    # 링크를 result 리스트에 추가\n",
    "    for link in news_link:\n",
    "        result.append(link.attrs['href'])\n",
    "    \n",
    "\n",
    "# 제목과 본문을 JSON 형식으로 저장\n",
    "articles = []\n",
    "for j in result:\n",
    "    response = requests.get(j) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.select_one('h2.media_end_head_headline').text\n",
    "    content = soup.select_one('article#dic_area').text.strip().replace(\"\\n\", '')\n",
    "        \n",
    "     # 기사 정보를 딕셔너리 형태로 저장\n",
    "    article = {\n",
    "        \"제목\": title,\n",
    "        \"내용\": content\n",
    "    }\n",
    "    articles.append(article)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('news_articles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 기사가 'news_articles.json' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사가 'news_articles.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json  \n",
    "\n",
    "# 링크 가져오기\n",
    "cursor = 0\n",
    "links = []\n",
    "for i in range(5):\n",
    "    URL = f'https://news.naver.com/section/template/SECTION_ARTICLE_LIST?sid=100&sid2=&cluid=&date=&next={cursor}'\n",
    "    response = requests.get(URL)\n",
    "    i = response.json()\n",
    "    text = text = i['renderedComponent']['SECTION_ARTICLE_LIST']\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    news_link = soup.select('.sa_text a[data-rank]')\n",
    "    cursor = soup.select_one('div._PERSIST_META')['data-cursor']\n",
    "    \n",
    "    for link in news_link:\n",
    "        href = link.get('href')  # href 속성 가져오기\n",
    "        links.append(href)\n",
    "        \n",
    "   # links의 내용을 줄바꿈으로 구분하여 출력\n",
    "    output = '\\n'.join(links)\n",
    "\n",
    "# 제목과 본문을 JSON 형식으로 저장\n",
    "articles = []\n",
    "for j in links:\n",
    "    response = requests.get(j) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.select_one('h2.media_end_head_headline').text\n",
    "    content = soup.select_one('article#dic_area').text.strip().replace(\"\\n\", '')\n",
    "        \n",
    "     # 기사 정보를 딕셔너리 형태로 저장\n",
    "    article = {\n",
    "        \"제목\": title,\n",
    "        \"내용\": content\n",
    "    }\n",
    "    articles.append(article)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('news_articles2.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 기사가 'news_articles.json' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
